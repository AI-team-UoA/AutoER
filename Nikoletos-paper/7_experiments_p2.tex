\section{Experimental Evaluation - Problem 2}
\label{sec:expProblem2}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{Nikoletos-paper/figures/Figure7.png}
    % \vspace{-10pt}
    \caption{The F1-score of (a) the trained RF, and (b) the trained AutoML model per dataset and instance generator. GB-F1 stands for the globally best F1-score among the search algorithms in Tables \ref{tab:best-gridesearch-trials} and \ref{tab:global-bestf1s}, i.e., the performance of the best search pipeline.}
%    \vspace{-14pt}
    \label{fig:rfAutoML}
\end{figure*}
%\vspace{-3pt}
\subsection{Experimental Setup}\label{ssec:setup-p2}
For the implementation of Random Forest, we used scikit-learn v. 1.4.2\footnote{\underline{https://scikit-learn.org}}. 
For
%To implement the 
AutoML, we used auto-sklearn v. 1.4.2\footnote{\underline{https://automl.github.io/auto-sklearn}} with the following parameters: (i) The time limit in seconds for the search of appropriate models (parameter: time\_left\_for\_this\_task) was set to 12 hours. (ii) The time for a single call in the ML model (parameter: per\_run\_time\_limit) was set to 4 hours. (iii) The memory limit in MB for the machine learning algorithm (parameter: memory\_limit) equal to  24.5 GB.
%6144$\cdot$4 MB. 
(iv) The number of jobs to run in parallel (parameter: n\_jobs) was set to 1.
%No validation set used on this task, as auto-sklearn creates one internally.
All experiments were executed on a server running Ubuntu 22.04, equipped with Intel Xeon E5-4603 v2@2.2GHz and 16GB~RAM.

%\vspace{-3pt}

\subsection{Evaluation Results}
\label{sec:tackleProblem2}
%\vspace{-3pt}
To fine-tune the workflow in Figure~\ref{fig:eeter_pipeline} without any indication of matches, we apply the following procedure, resembling the leave-one-out cross-validation approach: for each dataset $D_x$ in Table~\ref{tab:dataset-specs}, we use as training set all instances generated by grid and/or sampling-based search for all other nine datasets (i.e., all datasets among $D_1$ and $D_{10}$, except $D_x$). Using these labelled instances, we train a regression model using one of the approaches in Section \ref{sec:learningProcess}. Then, we apply the learned regression model to all instances generated by the same approach for $D_x$, estimating the respective F1-score. The instance corresponding the maximum predicted F1 provides the configuration features for the ETEER pipeline that will be eventually applied to $D_x$ in order to compute the actual F1-score.

It is important to clarify the following points: 
(i) In order to find the optimal parameters, we train a model that \emph{predicts} F1-scores per configuration and picks the best one. 
(ii) After picking the best configuration, we evaluate it by computing the \emph{actual} F1 score.
(iii) Thus, we are not actually interested in the accuracy of our F1-score predictions; we are only interested in the actual F1-scores computed by using our suggested configurations.

%Note that we are not interested in measuring the accuracy of the learned model in predicting the actual F1-score per configuration; our goal is to ensure that the optimal parameter configuration is assigned the maximum predicted F1.

In this context, we address the following research questions:
\begin{enumerate}[leftmargin=*, label=RQ\arabic*), start=1]
    \item Do the three instance generation approaches in Section \ref{sec:instanceGeneration} affect the performance of the learned models?
    \item Which of the two learning processes in Section \ref{sec:learningProcess} exhibits the highest effectiveness and time efficiency?
    \item Do all features in Section \ref{sec:datasetProfiling} contribute to the effectiveness of the solutions to Problem 2?
    \item Do  our solutions to Problem 2  outperform the baseline methods w.r.t. effectiveness and time efficiency?
    \item Do our solutions to Problem 2 generalize to an unseen dataset of completely different characteristics and scale? 
    \item {How do our approaches perform in comparison to established tools for end-to-end ER that leverage syntactic similarities?}
\end{enumerate}
We elaborate on these research questions in the following.

%To address RQ4, we introduce an extra dataset (in addition to the 10 ones in Section~\ref{sec:experiments-p1}): D11, a large heterogeneous dataset which matches two different versions of DBpedia that chronologically differ by 3 years \cite{DBLP:journals/is/PapadakisMGSTGB20}.
%As baselines, we use three approaches:

%\begin{enumerate}[leftmargin=*]
%    \item The default pipeline in Table \ref{tab:best-gridesearch-trials}.
%    \item The best search pipeline, i.e., the one with the best performance for a specific dataset among the grid and sampling-based search algorithms in Tables \ref{tab:best-gridesearch-trials} and \ref{tab:global-bestf1s}, respectively. It represents the best possible performance for the ETEER pipeline of Figure~\ref{fig:eeter_pipeline}.
%    \item ZeroER \cite{DBLP:conf/sigmod/WuCSCT20}, an established ETEER approach that involves both Filtering and Verification, while requiring no labelled instances for the dataset at hand. Its performance is reported in Table \ref{tab:zeroer-results}. Note that it did not terminate in three datasets within 2 days.
%\end{enumerate}

%\begin{figure*}[t]
%    \centering
%    \begin{minipage}{0.44\linewidth}
%        \centering
%        \includegraphics[width=\linewidth]{figures/predictions/sklearn_and_nn_f1.png}
        %\caption{The F1-score of the trained LR model per dataset and instance generator.}
        %\label{fig:lr-nn-f1s}
%    \end{minipage}
%    \hfill
%    \begin{minipage}{0.44\linewidth}
%        \centering
%        \includegraphics[width=\linewidth]{figures/predictions/autosklearn_f1.png}
        %\caption{The F1-score of the trained AutoML model per dataset and instance generator.}
%        \label{fig:autosklearnf1s}
%    \end{minipage}
%    \vspace{-10pt}
%     \caption{{\color{red}The F1-score of (a) the trained LR, and (b) the trained AutoML model per dataset and instance generator. GB-F1 stands for the globally best F1-score among the search algorithms in Tables \ref{tab:best-gridesearch-trials} and \ref{tab:global-bestf1s}, i.e., the performance of the best search pipeline.}}
%     \vspace{-10pt}
%\end{figure*}

\textbf{RQ1.} Figure \ref{fig:rfAutoML}(a) reports the F1-score of RF per dataset and instance generation approach. We observe that {the differences between the three instance generators are statistically insignificant ($p\gg0.05$ for all three pairs of generators)}. Indeed, the difference between the maximum and the minimum F1 among the three instance generators is far below 1.5\% in six of the datasets, raising to just 2.6\% and 3.6\% in $D_5$ and $D_7$, respectively. This means that there are only two datasets with significant differences between the three approaches: $D_6$, where sampling-based search takes a clear lead, and $D_{10}$, where grid search is the only approach with high performance. The extremely low performance of sampling-based search and \textit{all} on $D_{10}$ leads to \textit{grid search constituting the overall best approach for RF}, as it exhibits the highest mean F1 and the lowest variance.
%, while its distance from the globally best F1 is 10\%, on average. 
%The only exceptions appear in $D_6$ and $D_5$, where the sampling-based instances perform slightly worse and slightly better, respectively. This means that LR is able to leverage the information encapsulated in the dataset features proposed in Section \ref{sec:datasetProfiling}, regardless of the instance generator.

Much lower robustness with respect to instance generation is observed for AutoML, as shown in Figure \ref{fig:rfAutoML}(b). There is negligible deviation between the three approaches only in the two bibilographic datasets ($D_4$ and $D_9$), where the difference between the maximum and the minimum F1 is lower than 1\%. In $D_8$, this difference raises to 6\%, while in the remaining six datasets, it ranges from 12\% to 45\%. Interestingly, there is a clear winner among the three approaches: \textit{sampling-based search achieves the highest F1 in most of the datasets, while scoring the highest mean F1 and the lowest variance.} 
%On average, it also underperforms the globally best F1 by 11\%, while the other two approaches by more than 18\%.
%but $D_7$, where they rank third. In contrast, grid search instances excel in four datasets, but significantly outperform the sampling-based ones only in $D_7$.
{Indeed, the differences between sampling-based search and the other two generators are statistically significant ($p<0.05$), unlike the difference between all and grid search.}
This means that for AutoML, the 18,000 sampler instances (9 datasets $\times$ 4 search algorithms $\times$ 100 trials $\times$ 5 seeds) convey more useful information and less noise than the 359,100 grid search instances (9 datasets $\times$ 39,900 trials). 
%The combination of the two generators (``all'') typically follows the top performer or even outperforms both generators (in $D_7$).

%\input{Nikoletos-paper/tables/automl_f1s_with_data_features}

\input{Nikoletos-paper/tables/automl_f1s_only_sampling}

%\input{Nikoletos-paper/tables/lr_with_data_features_performance}

\input{Nikoletos-paper/tables/lr_with_data_features_only_gridsearch}

\textbf{RQ2.} We now compare the two learning processes in combination with their best instance generation approach. The performance of Random Forest with grid search instances along with the selected workflow configurations 
%for the workflow in Figure \ref{fig:eeter_pipeline} 
is reported in Table \ref{tab:lr-with-data-features}. The same information for AutoML in combination with sampling-based instances is reported in Table \ref{tab:autosklearn-results}. We observe significant variations between the selected configurations of the two learners for each dataset.

Regarding their effectiveness, the two learners exhibit practically identical F1 score in half the datasets. In fact, the difference in their F1 score is far less than 1\% in $D_1$, $D_3$, $D_4$, $D_8$ and $D_9$. AutoML takes the lead in $D_2$ and $D_6$, while Random Forest outperforms it in the three remaining datasets. {Overall, their difference is statistically insignificant ($p = 0.50651$).} With respect to average F1 score, Random Forest takes a significant lead (66.1 vs 64.3), while exhibiting lower variance. It should be stressed that \textit{AutoML's performance depends heavily on the available search/training time}, with higher budgets yielding even better results. Yet, the current limit of 12 hours is already too high when compared to the ETEER runtime, which does not exceed 36 seconds in any of the datasets.

Regarding time efficiency, Random Forest is clearly the top performer. Its training time is consistently lower than two seconds, unlike the 12 hours required by AutoML. Its prediction time is also extremely fast, consistently requiring less than 20 milliseconds, whereas AutoML typically takes a few minutes. This should be attributed to the simpler models learned by Random Forest, unlike the complicated, weighted ensemble learned by AutoML. 

For these reasons, Random Forest trained on grid search instances is the preferred approach to tackle Problem 2 and answer RQ3. Based on RF, the overall approach for training the regression model on an existing dataset, generating the testing instances to yield a workflow configuration and applying it to a specific dataset takes less than three minutes in all datasets of Table \ref{tab:lr-with-data-features}. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{Nikoletos-paper/figures/gini.png}
    % \vspace{-12pt}
    \caption{The gini importance of each feature per dataset for Random Forest with grid search instances.}
%    \vspace{-18pt}
    \label{fig:gini}
\end{figure}

\input{Nikoletos-paper/tables/zeroer}

\textbf{RQ3.} A major advantage of Random Forest is the interpretability of its trained decision trees, which allows for examining the specific features that are actually useful in predicting the best workflow configuration per dataset. In fact, Gini importance provides normalized estimations of the importance of each feature in $[0,1]$, with higher values indicating more important features. 

Figure \ref{fig:gini} reports this measure for all features of Section \ref{sec:datasetProfiling} across all datasets when training Random Forest with grid search instances. \textit{All features have a non-zero importance} that fluctuates between 0.137 and 0.014. On average, the most important features are F10 (0.095), F7 (0.061) and F6 (0.056), while the least important ones are F4 0.032) and F3 (0.035). Hence, \textit{despite the seemingly small variations, some features are two or even three times more important than others}. Random Forest inherently addresses these variations without requiring a specialized feature selection approach.

\input{Nikoletos-paper/tables/dbpedia_f1s}

\textbf{RQ4.} We now compare the top learned models, i.e.,  RF in combination with grid search instances and AutoML in combination with sampling-based search, with the following three baselines methods:

(i) The default pipeline in Section \ref{sec:tackleProblem1}. In most datasets, this approach underperforms both RF and AutoML to a significant extent, due to its fixed configuration. The only exception is $D_2$, where the default configuration is almost the same as the best one. AutoML performs much worse than the default configuration in $D_{10}$, too, due to the high levels of noise and the low portion of top performing configurations (see Figure \ref{fig:f1_boxplot_all}), which call for much higher search times. Overall, \textit{the adaptive workflow configurations proposed by RF and AutoML typically outperform the top-performing default one}.

(ii) The best search pipeline, i.e., GB-F1 in Figure \ref{fig:rfAutoML}, which corresponds to the best performance of the ETEER pipeline in Figure~\ref{fig:eeter_pipeline} for a specific dataset among the grid and sampling-based search algorithms in Tables \ref{tab:best-gridesearch-trials} and \ref{tab:global-bestf1s}, respectively.
%, which represents the best possible performance for the ETEER pipeline of Figure~\ref{fig:eeter_pipeline}.
%The second baseline one is the globally best F1-score among the search algorithms in Tables \ref{tab:best-gridesearch-trials} and \ref{tab:global-bestf1s} (i.e., GB-F1 in Figure \ref{fig:rfAutoML}). 
We observe that neither RF in combination with grid search instances nor AutoML in combination with sampling-based search instances outperform GB-F1 in any dataset. Yet, both RF and AutoML almost match GB-F1 in three datasets: $D_3$, $D_4$ and $D_9$. This is expected for the last two, the relatively clean bibliographic datasets, given the large portion of workflow configurations with very high performance in Figure \ref{fig:f1_boxplot_all}. For $D_{3}$, this shows the high effectiveness of both RF and AutoML. The worst performance of RF and AutoML with respect to this baseline corresponds to $D_1$ and $D_8$, where their F1-score is lower by $\sim$30\% and $\sim$20\%, respectively. These are the datasets with the lowest portion of duplicate entities, causing the selected workflow configurations to suffer from low precision. AutoML performs poorly in $D_{10}$, too, for reasons explained above.
%due to the high levels of noise and the low portion of top performing configurations (see Figure \ref{fig:f1_boxplot_all}), which call for much higher search times. 
All other datasets lie in the middle of these two extremes, with RF and AutoML underperforming GB-F1 by 10\% and 11\%, on average, respectively. 

%Random Forest, its best performance per dataset is reported in Table \ref{tab:lr-with-data-features}. It is very close to the best search pipeline in half the datasets, with LR achieving the same performance in the relatively clean bibliographic datasets, $D_4$ and $D_9$, as well as in $D_{10}$. In $D_2$ and $D_3$, its F1-score is less than 5\% lower than that of the best search pipeline. In contrast, LR underperforms almost by 30\% in $D_1$ and $D_8$, which are quite challenging, due to the very low portion of top performing configurations, as shown in Figure~\ref{fig:f1_boxplot_all}. In the remaining datasets, its F1-score is lower than the best search one by 23\% ($D_6$ and $D_8$) and 12\% ($D_5$). Overall, \textit{LR provides a performance close to the best possible one for the workflow in Figure~\ref{fig:eeter_pipeline} in most cases, while its run-time is consistently very low}, up to 21 seconds over the largest dataset, $D_{10}$. It is worth noting at this point the variety of parameter configurations proposed by LR in
%: as shown in 
%Table \ref{tab:lr-with-data-features}.
%, its overall run-time is consistently lower than the baseline methods in all datasets but $D_6$, where it uses the maximum number of candidates per query entity (100 vs 1 of the baseline methods).
%respet ETEER run-times in Table \ref{tab:best-gridesearch-trials} and most ones in Table~\ref{tab:global-bestf1s}.

(iii) ZeroER \cite{DBLP:conf/sigmod/WuCSCT20}, an established ETEER approach that involves both Filtering and Verification, while requiring no labelled instances for the dataset at hand. Its performance is reported in Table \ref{tab:zeroer-results}. Note that it did not terminate in three datasets within 2 days.
Compared to ZeroER, RF is consistently much faster: it requires far less than 3 minutes in all cases, while ZeroER requires at least 26 min in all datasets but the smallest one, where it actually finds no matches. The reason is that ZeroER cannot support missing and misplaced values, which abound in $D_1$. Apart from this dataset, RF significantly outperforms ZeroER in terms of effectiveness in three more datasets: $D_2$, $D_4$ and $D_9$. The reason is that those datasets convey long textual values, which are ideal for the pre-trained language model that lies at the core the ETEER pipeline in Figure \ref{fig:eeter_pipeline}. In contrast, $D_5$, $D_7$ and $D_{10}$ convey short textual values that usually correspond to person names. The language models struggle to find semantic similarities in these settings, unlike the string similarity measures that lie at the core of ZeroER. As a result, RF significantly underperforms ZeroER in these three datasets, but remains faster by orders of magnitude.

%Regarding AutoML, its performance is reported in Table \ref{tab:autosklearn-results}. Apart from the much higher training time (12 hours), it also raises the prediction time, due to the much more complex model that results from its learning. As a result, its performance is higher than LR in most cases, i.e., in six datasets; in $D_1$, $D_2$ and $D_4$, they yield the same F1, and only in $D_{10}$ LR performs better. Compared to the best search pipeline, the F1 score of its fine-tuned ETEER pipelines is lower by (far) less than 10\% in seven datasets. The difference is significant only in $D_1$ and $D_8$, due to the very low portion of top performing pipelines, as well as in $D_{10}$. 

Compared to ZeroER, AutoML achieves higher F1-score in the same four datasets as RF, while undeperforming in the same three datasets, for the same reason (i.e., the length of attribute values). Yet, AutoML is much worse than ZeroER in $D_5$ and $D_7$, while its run-time is significantly higher than ZeroER in all datasets, but $D_9$ (and the three datasets where ZeroER runs for more than 48 hours). 

%Overall, both RF and AutoML are capable of approximating the performance of the fine-tuned ETEER pipeline in Figure \ref{fig:eeter_pipeline} in most cases. RF emphasizes run-time, minimizing the training and prediction times, while AutoML emphasizes effectiveness. Compared to ZeroER, 
Overall, \textit{the relative effectiveness of RF and AutoML with respect to ZeroER depends on data characteristics. RF is more scalable and time efficient, while AutoML has a similar, but adjustable run-time}.
%in the case of AutoML. 

%{\color{red}It is worth noting that the overall runtime and the runtime per model, which are auto-sklearn parameters, are dependent to the resulting performance. More time provided will result probably in an even better model selected. In our experiments, we used the predefined time limits and did not further investigate the impact of longer runtimes on AutoML's ability to find the optimal model. F}

\textbf{RQ5.} In this experiment, we evaluate the generalization of the solutions to Problem 2 on $D_{11}$, a dataset with characteristics that are substantially different from all other datasets in Table \ref{tab:dataset-specs} -- unlike the limited size and schema of the other datasets, it contains millions of heterogeneous entities with user-generated content using 50,000 different attributes from two versions of DBpedia that chronologically differ by 3 years \cite{DBLP:journals/is/PapadakisMGSTGB20}.
%Wikipedia and more than \cite{DBLP:journals/is/PapadakisMGSTGB20},
Due to its size, every run of the ETEER pipeline in Figure \ref{fig:eeter_pipeline} typically takes $\sim$12 hours.
%on $D_{11}$. As a result, 
Hence, ZeroER and grid search are inapplicable,
%to this dataset, 
while the sampling-based approaches of Section~\ref{sec:problem-1} are extremely time consuming. Therefore, we use the default configuration defined in Section \ref{sec:tackleProblem1} as baseline.

Table \ref{tab:dbpedia-results} reports the performance of this baseline
%method 
along with 
%the two learning processes, i.e., 
RF and AutoML in combination with all instances generated from $D_1$-$D_{10}$ by the three approaches in Section \ref{sec:instanceGeneration}. 
%This affects only AutoML, as RF selects the same parameter configuration, regardless of the instance generator.
All tested pipelines exhibit relatively high effectiveness, with the default one matching the performance of the fine-tuned pipeline in \cite{DBLP:journals/is/PapadakisMGSTGB20}, which leverages traditional, string similarities for Filtering and Verification. This suggests that DBpedia entails many top performing configurations, like $D_4$ and $D_9$ in Figure~\ref{fig:f1_boxplot_all}. Most importantly, the configurations proposed by RF and AutoML exhibit higher F1-score by 1.6\% -- the only exception is AutoML with grid search instances, which significantly underperforms the baseline method, probably because it  requires a higher search time. This verifies the high effectiveness of the solutions to Problem 2 even in settings significantly different from the datasets generating the training instances. 

In terms of time efficiency, the prediction times of both RF and AutoML are quite low, while the run-time of the automatically configured workflows is comparable to that of the baseline.
%(default configuration). 
In fact, the sampling-based instances yield lower ETEER times by 8.9\% for both RF and AutoML. The latter, though, exhibits very high training times (12 hours), yielding a much higher overall run-time.
%Despite its higher training and prediction time and the more complex lsearned model, AutoML cannot supersede RF; in combination with sampling-based and with all instances, it exhibits the same effectiveness at the cost of higher run-times. In combination with grid search instances, the F1-score of AutoML is significantly lower.

%We can conclude, therefore, that both 
Overall, \textit{RF and AutoML 
%are able to 
can automatically configure 
%a top performing 
the ETEER pipeline even for a voluminous dataset with high levels of noise and schema heterogeneity}, with RF exhibiting very high time efficiency.
%, too.

{
\textbf{RQ6.} We now compare our approaches with two established open-source ER tools that implement end-to-end pipelines based on syntactic similarities: Magellan \cite{DBLP:journals/pvldb/KondaDCDABLPZNP16}, which involves a supervised functionality, and JedAI \cite{DBLP:journals/is/PapadakisMGSTGB20}, which operates in a learning-free way. 

For Magellan, we apply overlap blocking to the input entity collections, yielding the set of candidate pairs. These pairs are then vectorized into feature vectors, with each dimension corresponding to the score of a different similarity threshold. 80\% of these candidate pairs are automatically labelled based on the ground truth and used to train a Random Forest classifier. The trained model is then evaluated on the remaining 20\% of the candidate pairs.

For JedAI, we consider the default configuration proposed in \cite{DBLP:journals/is/PapadakisMGSTGB20} after an extensive experimental study. It consists of the following sequence of methods (note that the first two are parameter-free): (i) Token Blocking, (ii) Block Purging, (iii) Block Filtering with ratio $0.8$, (iv) CNP with the Jaccard similarity as weighting scheme, (v) Matching with character four-grams, TF-IDF weights and cosine similarity, (vi) Unique Mapping Clustering with similarity threshold~0.1.

The results are compared with the best solutions to Problems 1 and 2 in Table \ref{tab:baselines}. Starting with the best sampling-based solution to Problem 1, 
%we observe that 
its average F-Measure exceeds those of Magellan and JedAI by 5\% and 4\%, respectively. In fact, Best Sampling outperforms Magellan in half the datasets (D2-D5, D9) and exhibits almost the same performance in two others (D6, D7), with Magellan taking a major lead only in D8. Compared to JedAI, Best Sampling underperforms it only in D10, while exhibiting the same performance in D2, D3 and D4; in the remaining seven datasets, Best Sampling achieves much higher performance than JedAI.
%while dominating in seven datasets -- in the remaining datasets , they almost have .

Regarding Best AutoML and Best RF, both underperform Magellan with respect to the average F-Measure to a minor extent (by 3.7\% and 1.9\%, respectively). Looking into the individual datasets, though, we observe that there is a balance between Magellan and our approaches, as Best AutoML and Best RF are the top performers in half the datasets (D2-D5, D9). This is remarkable, when considering that Magellan requires labels for the vast majority of the candidate pairs, whereas our approaches rely exclusively on evidence from other, already labelled datasets. As a result, Best AutoML and Best RF scale easily to D11, unlike Magellan. 

\input{Nikoletos-paper/tables/baselines}

Finally, comparing JedAI with Best AutoML and Best RF, we observe that the former achieves a higher average F-Measure only because of the extremely higher performance on D10. Excluding D10, the relative performance is reversed, with our approaches outperforming JedAI by at least 0.6\%: 66.71 for JedAI vs 67.41 for Best AutoML and 67.31 for Best RF. The advantage of our approaches becomes greater when considering D11, too, where JedAI underperforms them by 3.6\% (81.3 vs 84.9 for our approaches). Looking at the individual datasets, we observe that there is a balance between JedAI and our approaches. They all have almost the same performance on D4, with JedAI being the top performer in 5 datasets and our approaches in the remaining five ones (including D11). Apart from the high performance, Best AutoML and Best RF have a qualitative advantage, as their functionality is adaptive, depending on the characteristics of the input data, unlike JedAI, which applies the same pipeline to all datasets.

On the whole, our approaches significantly outperform Magellan and JedAI when labelled data is available, otherwise they offer similar levels of performance. In the latter case, though, they scale well to large datasets, unlike Magellan, while providing an adaptive, data-driven functionality, unlike JedAI.
}

\textbf{Conclusions.} Both RF and AutoML 
%Our methodology to addressing Problem 2 
are capable of fine-tuning the ETEER pipeline in Figure \ref{fig:eeter_pipeline} under versatile settings. Using the former (ideally with grid search instances), the training and prediction times are minimized, while the proposed configuration typically achieves higher effectiveness than the one proposed by AutoML, despite its time-consuming search phase. 
%the  performs very well in most cases. AutoML trades significantly higher effectiveness for much higher training and prediction times, but this can be adjusted according to the available time and memory.%temporal resources.

%=================================
%As outlined in Section~\ref{sec:learningProcess}, when the ground truth is not available, we employed two different approaches. The primary strategy for tackling Problem 2 was to utilize the trials conducted for Problem 1, in conjunction with a set of data features extracted from the datasets to be matched. We organized the experiments for this task by first dividing the trials from Problem 1 into three distinct categories: \textit{Optuna}, which comprises the combined trials from all four Optuna samplers; \textit{Gridsearch}, which consists of all trials from the comprehensive grid search benchmarking; and \textit{all}, which is the amalgamation of both the \textit{Optuna} and \textit{Gridsearch} trials. Using these three sets, we trained models on each set following the two learning procedures described in Section~\ref{sec:learningProcess}. Specifically, we obtained 52,500 trials from each sampler, totaling 5*52,500 = 210,000 trials for the \textit{Optuna} set. The \textit{Gridsearch} set contained 399,000 trials (39,900 trials for each of the 10 datasets). Combining these, the \textit{all} set comprised 609,000 trials.
%For all the training experiments, trials with an F1-score of zero were excluded from the training sets. Additionally, duplicate trials, where samplers proposed configurations already covered by the grid search, were also removed from the training sets. 
% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/predictions/sampler_distribution.png}
%     \caption{All trials training set distribution.}
%     \label{fig:sampler-distributions}
% \end{figure}



%\input{Nikoletos-paper/tables/default_conf_performance}


%\subsubsection{AutoML learning procedure}

%Using auto-sklearn, we conducted experiments by withholding one dataset as the test set while training the AutoML models on all remaining datasets. The results of this approach are summarized in Table \ref{tab:autosklearn-results}, where we present the best experiments for each dataset. The column labeled \textit{Test set} indicates the dataset used as the test set, while all others served as training sets. The \textit{Trials} column lists the set of trials used for each experiment. \textit{Predicted F1} refers to the actual F1-score of the predicted configuration, measured using pyJedAI. \textit{GB-F1} represents the highest F1-score obtained by any method for each dataset, and \textit{Performance} measures how close the ETEER prediction was to the best F1-score. 
%\comments{psiliazomai, mallon swsta, ti einai to performance. mipws na dwseis ton tupo pou xreisimopoieis? vasilis: sigoura!}

%\input{Nikoletos-paper/tables/automl_f1s_without_data_features}


%Among all AutoML attempts, implementations of Gradient Boosting and Extra Trees models consistently showed the best performance. Additionally, using the \textit{all} trials set yielded the best results for 6 out of 10 datasets. However, it is also notable that the \textit{Optuna} trials sets produced the best results in 4 out of 10 datasets.

%The configurations predicted by AutoML achieved near-optimal F1-scores, particularly for datasets D2, D3, D4, D5, D7, and D9, with the minimum \textit{Performance} being 69\% "close" to the best F1-score. To further illustrate the results of this learning procedure, Figure \ref{fig:autosklearn-f1s} provides a comparative analysis of all datasets across all trial sets. There is no significant distinction between the sets of trials, as they exhibit similar performance, which is encouraging as it suggests robustness across different trial sets.
%\comments{Sto d2 kai d10 exoume kapoies diafores. eksigise giati. kapoia xaraktiristika twn datasets isws?}



%It is worth noting that the overall runtime and the runtime per model, which are auto-sklearn parameters, are dependent to the resulting performance. More time provided will result probably in an even better model selected. In our experiments, we used the predefined time limits and did not further investigate the impact of longer runtimes on AutoML's ability to find the optimal model.

%\comments{genika den sxoliazeis ka8olou kati metaksu twn datasets. kati na pros8esoume? epireazoun px ta features sto section 5.1?}

%\subsubsection{Individual regressors learning procedure}

%In a different manner, we also employed another learning procedure with quite a different methodology. We tested classic regressors provided from sklearn open-source python toolkit, while also created a naive Neural Network to test it for this task. A validation set of size 10\% from all training data trials is created, and using this we try to minimize the MSE using Optuna as a hyperparameter tool. To better understant this technique please revisit Section \ref{sec:learningProcess} and Learning Procedure 2. The exact configuarations tested can be found in Appendix and Table \ref{tab:parameter-values}. 

% \input{tables/sklearn_and_nn_f1s}

%\input{Nikoletos-paper/tables/lr_without_data_features} 


%Table \ref{tab:sklearn-nn-results}, shows the performance of this learning procedure. Again Random Forest is shown to be quite effective. In contrast with AutoML experiment, more trials (i.e. \textit{All} training set) yields in 6 out of 10 the best results. In a same way like the previous experiment, we provide Figure \ref{fig:sklearn-nn-f1s}, where again we notice no big differnece between sets of trials. It is now evident that no matter the trials set we will get a similar scrore, making Optunas trials set, really appealing as it has less than half the size of \textit{All} trials set.


% \subsubsection{Comparative analysis ??}

% \input{tables/all_f1s}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.99\linewidth]{figures/predictions/all_f1.png}
%     \caption{All F1-scores.}
%     \label{fig:all-f1s}
% \end{figure}

